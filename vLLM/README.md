# vLLM

vLLM is an open-source inference and serving engine designed to optimize the performance of large language models (LLMs). It achieves high throughput and memory efficiency with optimizations like PagedAttention, Dynamic Batching etc leading to Efficient Resource Utilization.

* [vLLM Github Repo](https://github.com/vllm-project/vllm)
* [General Documentation for Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)

Platform Specific Instuctions and scripts used for LLM-Inference-Bench

* [Nvidia A100](./A100/)
* [Nvidia H100](./H100/)
* [Nvidia GH200](./GH200/)
* [AMD MI250](./MI250/)
* [AMD MI300X](./MI300X/)
* [Intel Max 1550](./Max1550/)
* [Habana Gaudi 2](./Gaudi2/)