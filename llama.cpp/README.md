# vLLM

llama.cpp is an open-source implementation of Meta's LLaMA architecture, written in C++. It is designed to facilitate efficient inference of large language models on various hardware platforms, including consumer-grade devices.

* [llama.cpp Github Repo](https://github.com/ggerganov/llama.cpp)
* [General Documentation for Installation](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)

Platform Specific Instuctions and scripts used for LLM-Inference-Bench

* [Nvidia A100](./A100/)
* [Nvidia H100](./H100/)
* [Nvidia GH200](./GH200/)
* [AMD MI250](./MI250/)
* [AMD MI300X](./MI300X/)
* [Intel Max 1550](./Max1550/)